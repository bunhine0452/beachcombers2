import streamlit as st
from css import hard_type_css
import os

from funcs import linegaro
def transformer_info():
    a,b,c,d = st.columns([0.05, 0.1, 0.01 ,1])
    with a:
        st.write('')
        st.image('https://kli.korean.go.kr/images/logo_kor_2.png')
    with b:
        st.write('')
        st.write('')
        st.image('https://kli.korean.go.kr/images/logo_kor_header.png', width=400)
    with d:
        st.title('Transformer 에 대하여')
    st.markdown('#')
    with st.expander('Before we go', expanded=True):
        st.markdown('''
                ### **Transformer?**
                
                Transformer는 자연어 처리(NLP)에서 가장 중요한 딥러닝 모델 중 하나로, 특히 번역, 문장 생성, 텍스트 분류 같은 작업에서 뛰어난 성능을 보입니다. 
                2017년에 "Attention is All You Need"라는 논문에서 처음 제안되었으며, 이후 많은 최신 언어 모델들이 Transformer 구조를 기반으로 만들어졌습니다.

                #### **핵심 개념**
                - **Attention 메커니즘**: Transformer는 "어텐션"이라는 메커니즘을 사용하여 문장 내에서 중요한 단어에 집중합니다. 예를 들어, 긴 문장에서 특정 단어가 다른 단어들과 어떻게 연관되어 있는지를 어텐션으로 파악합니다.
                - **병렬 처리**: 기존 순차적 모델(RNN, LSTM)과 달리, Transformer는 병렬 처리가 가능해 훈련 속도가 빠르고 더 긴 문장도 효과적으로 처리할 수 있습니다.
                - **Encoder-Decoder 구조**: Transformer는 인코더와 디코더로 구성됩니다. 인코더는 입력 문장을 처리하고, 디코더는 그 정보를 바탕으로 새로운 문장을 생성합니다. 번역 작업을 예로 들면, 인코더는 한국어 문장을 이해하고, 디코더는 영어 문장을 생성하는 방식입니다.
                ''')
    st.write('탭으로 이동')
    tab1, tab2 ,tab3, tab4= st.tabs(['RNN', 'Seq2Seq','Attention / Self-Attention','Transformer'])
    with tab1:
        st.image('https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg')
        st.markdown('''RNN은 컴퓨터가 순차적으로 데이터를 처리하는 방식 중 하나로, 주로 시간에 따라 변화하는 데이터를 다룰 때 유용합니다. 
                예를 들어, 문장의 단어를 차례로 처리하거나, 음성 데이터를 시간 순서대로 분석할 때 사용됩니다.''')
        p1 , p1_1 = st.columns([1,1])
        with p1:
            st.markdown('''
                        ### RNN 구조

                        1. **왼쪽의 기본 구조**:
                        - **h**는 **은닉 상태(hidden state)** 라고 부르는데, 이 값은 이전에 처리한 정보(데이터)를 기억하고 저장하는 역할을 합니다. RNN이 데이터를 처리할 때, 이전에 봤던 데이터를 계속 기억하면서 새로운 데이터를 이해하려고 합니다.
                        - **x**는 **입력 값**입니다. 예를 들어, 입력값이 문장의 단어라면, RNN은 각 단어를 순서대로 처리하게 됩니다.
                        - **o**는 **출력 값**입니다. RNN이 최종적으로 만들어내는 결과물입니다. 예를 들어, 텍스트를 입력받고 번역을 할 때 번역된 문장이 출력값이 됩니다.
                        - **w**와 **v**, **U**는 계산에 사용되는 값(가중치)들인데, 데이터가 어떻게 변하는지 결정하는 역할을 합니다.

                        2. **오른쪽 펼쳐진 구조 (Unfold)**:
                        - 그림의 오른쪽은 RNN이 여러 개의 입력을 시간 순서대로 처리하는 모습을 나타냅니다. 
                        - **h(t-1), h(t), h(t+1)** 은 각각 다른 시간에 처리된 데이터의 은닉 상태입니다. 이 값들은 순서대로 연결되며, 이전에 봤던 데이터의 정보가 다음 데이터 처리에 영향을 줍니다.
                        - **x(t-1), x(t), x(t+1)** 은 시간에 따라 들어오는 입력 값들입니다. 예를 들어, 첫 번째 단어가 **x(t-1)**, 두 번째 단어가 **x(t)**, 세 번째 단어가 **x(t+1)** 라고 할 수 있습니다.
                        - 이 값들이 연속적으로 연결되면서 RNN은 앞에서 처리한 데이터를 기억하며, 문장이나 데이터의 전체 흐름을 파악하려고 합니다.
                        ''')
        with p1_1:
            st.markdown('''    
                        ### 쉽게 이해하는 예
                        예를 들어, RNN이 "나는 사과를 먹었다"라는 문장을 처리한다고 해봅시다. 
                        1. 처음에 "**나는**"을 입력받으면, 이 단어를 기억하며, RNN의 은닉 상태는 "**나는**"이라는 정보를 저장합니다.
                        2. 다음으로 "**사과를**"을 입력받으면, RNN은 "**나는**"을 기억한 상태에서 "**사과를**"이라는 단어를 처리합니다.
                        3. 마지막으로 "**먹었다**"를 입력받으면, 앞서 기억한 "**나는 사과를**" 정보를 바탕으로 "**먹었다**"라는 단어를 이해하고 전체 문장의 의미를 파악하게 됩니다.

                        이렇게 RNN은 데이터를 순서대로 처리하며, 앞에서 받은 정보를 잊지 않고 기억하면서 새로운 데이터를 처리하는 것이 특징입니다. 이 덕분에 문장이나 음악처럼 시간에 따라 변화하는 데이터를 효과적으로 분석할 수 있습니다.
                        
                        ### RNN의 한계
                        RNN의 한계는 데이터가 길어질수록(예: 긴 문장) 앞부분의 정보를 점점 잊어버리기 쉽다는 점인데, 이를 개선하기 위해 더 나은 모델들이 나왔습니다. Transformer 모델도 이런 문제를 해결한 대표적인 기술 중 하나입니다.
                        ''')
    with tab2:
        st.image('https://incredible.ai/assets/images/seq2seq-seq2seq_ts.png', width=4000)
        st.markdown('''
                    Seq2Seq 모델은 하나의 시퀀스(입력 데이터)를 받아 또 다른 시퀀스(출력 데이터)를 생성하는 모델입니다. 이 모델은 인코더와 디코더로 구성되며, 
                    입력 데이터를 요약하고 그 요약된 정보를 바탕으로 출력을 생성하는 구조를 가지고 있습니다.
                    Seq2Seq는 주로 자연어 처리에서 두 개의 다른 시퀀스를 입력과 출력으로 변환하는 데 사용되며,
                    번역, 챗봇, 요약 등의 작업에 매우 유용합니다. 이 모델은 **인코더(Encoder)** 와 **디코더(Decoder)** 라는 두 가지 주요 부분으로 구성되어 있습니다.
                    ''')
        p2 , p2_1 = st.columns([1,1])
        with p2:
            st.markdown('''
                        ### 설명

                        1. **인코더(Encoder)**
                        - 왼쪽의 초록색 박스들로 구성된 부분입니다. 각 박스는 **GRU(Gated Recurrent Unit)** 라는 특별한 형태의 RNN 셀을 나타냅니다. 
                        GRU는 순차 데이터를 처리하는 데 사용되며, 데이터의 과거 정보를 기억하고, 중요한 정보를 남기며, 불필요한 정보는 잊어버리는 역할을 합니다.
                        - **x₁, x₂, ..., xₙ**은 입력 데이터(과거 데이터)를 나타냅니다. 이 입력 데이터는 순차적으로 GRU를 통과하면서, **인코더 상태(Encoder State)** 로 변환됩니다. 
                        이 상태는 인코더가 입력 데이터를 요약한 값으로, 디코더가 사용할 중요한 정보입니다.

                        2. **디코더(Decoder)**
                        - 오른쪽의 보라색 박스들로 구성된 부분입니다. 디코더 역시 GRU 셀로 구성되어 있으며, 인코더에서 전달받은 **인코더 상태**를 기반으로 새로운 시퀀스를 생성합니다.
                        - **y₁, y₂, ..., yₙ**은 출력 데이터(예측값)을 나타냅니다. 디코더는 인코더 상태와 자신의 예측값을 사용해 다음 값을 예측하는 방식으로 출력을 만듭니다.
                        - 예를 들어, 번역 작업에서는 입력 데이터가 한국어 문장이라면, 출력 데이터는 영어 문장이 될 수 있습니다.
                        ''')
        with p2_1:
            st.markdown('''
                        ### 작동 방식

                        1. **인코더**는 입력 시퀀스(문장, 텍스트, 시계열 데이터 등)를 받아 순차적으로 처리한 후, 그 데이터를 하나의 벡터(인코더 상태)로 요약합니다. 이 과정에서 입력 데이터의 패턴과 구조를 이해하게 됩니다.
                        2. **디코더**는 인코더로부터 받은 상태 벡터를 바탕으로 새로운 시퀀스를 만듭니다. 이때, 디코더는 이전에 예측한 값을 사용해 다음 값을 계속해서 예측합니다.

                        ### 예시:
                        Seq2Seq 모델은 번역 작업에서 자주 사용됩니다.
                        - **인코더**는 한국어 문장을 읽고, 그 정보를 요약합니다.
                        - **디코더**는 요약된 정보를 바탕으로 영어 문장을 하나씩 만들어냅니다. "나는 학교에 간다"라는 문장을 입력받았다면, 디코더는 "I go to school"이라는 문장을 차례차례 생성합니다.

                        ### GRU
                        GRU는 RNN의 변형 중 하나로, 데이터의 중요한 부분을 기억하고, 덜 중요한 부분은 잊는 기능을 자동으로 조절하는 장치입니다. RNN이 긴 데이터를 처리하면서 정보를 잊는 문제를 해결하기 위해 고안된 셀입니다. GRU는 **LSTM**과 비슷하지만, 구조가 더 단순해 빠르게 처리할 수 있습니다.

                        ''')
        st.markdown('''
                    
                    ### Seq2Seq의 한계와 Attention의 도입 이유

                    1. **Seq2Seq의 문제점**:
                    - Seq2Seq에서 인코더는 입력 시퀀스를 **하나의 고정된 벡터**로 압축합니다. 이 벡터는 입력 시퀀스의 모든 정보를 담고 있어야 하며, 디코더는 이 벡터만을 참고해 출력을 생성합니다.
                    - 문제는 **긴 문장** 을 처리할 때 발생합니다. 모든 정보를 하나의 벡터에 담기 때문에, 긴 문장의 경우 앞부분의 정보가 뒤로 갈수록 소실될 수 있습니다. 
                      이로 인해, 출력 문장 생성 시 중요한 정보가 잘 전달되지 못할 수 있습니다.

                    2. **Attention의 도입**:
                    - **Attention** 은 Seq2Seq의 이 한계를 극복하기 위해 도입되었습니다. 인코더가 입력 시퀀스의 각 단어를 처리할 때, 단순히 하나의 벡터로 요약하지 않고, **모든 단어 정보를 별도로 유지** 합니다.
                    - 디코더는 출력 시퀀스를 생성할 때, 인코더의 각 단어와의 **연관성을 계산**합니다. 이때, **Attention 메커니즘** 은 디코더가 각 입력 단어가 얼마나 중요한지를 평가할 수 있게 도와줍니다.
                    
                    ### Attention은 어떻게 계산을 보완하는가?

                    - Seq2Seq는 입력 전체를 한 번에 압축하는 방식이라면, **Attention** 은 디코더가 출력의 각 단계에서 **모든 입력 단어에 주목** 할 수 있게 해줍니다.
                    - Attention은 입력 시퀀스의 모든 단어가 **출력 시퀀스의 각 단어에 대해 얼마나 중요한지 계산** 하여, **중요한 단어에 더 큰 가중치를 부여** 합니다. 
                    - 이 과정은 Attention의 **쿼리(Query)**, **키(Key)**, **값(Value)** 를 통해 이루어지며, 단어 간의 상관성을 수학적으로 계산하여 더 나은 출력을 생성할 수 있게 만듭니다.

                    ### 정리

                    Seq2Seq는 기본적으로 데이터를 순차적으로 처리하는 강력한 모델이지만, **긴 문장**에서 정보 손실이 발생할 수 있는 단점이 있었습니다. 
                    **Attention 메커니즘**은 이러한 한계를 극복하기 위해 **디코더가 입력 시퀀스 전체를 동적으로 참조**할 수 있게 만들었고, 그 결과 중요한 정보를 놓치지 않고 잘 처리할 수 있게 되었습니다. 
                    ''')
        with tab3:

            with st.expander('**Attention**'):
                st.image('https://wikidocs.net/images/page/22893/%EC%BF%BC%EB%A6%AC.PNG')
                st.markdown('''
                        **어텐션(Attention)** 은 모델이 입력 데이터 중에서 가장 중요한 부분을 선택하고 집중하는 **메커니즘** 입니다. 
                        즉, 모든 입력 데이터를 동일하게 처리하지 않고, 특정 단어가 다른 단어들과 얼마나 관련이 있는지 계산한 후, 
                        중요한 정보에 더 많은 가중치를 부여하여 처리하는 방식입니다. 이를 통해 모델은 긴 문장이나 복잡한 데이터를 더 효과적으로 이해하고, 
                        필요한 정보에 더 잘 집중할 수 있게 됩니다.
                        ''')

            with st.expander('**Self-Attention**'):
                st.markdown('''

                        셀프 어텐션은 문장의 각 단어가 다른 단어들과 얼마나 관련 있는지를 계산하여, **중요한 단어에 더 집중**할 수 있도록 하는 메커니즘입니다. 이 메커니즘을 통해 모델은 문장 내에서 단어 간의 상호 관계를 효과적으로 파악할 수 있습니다.

                        1. **병렬 처리 가능**:
                        RNN과 달리, 셀프 어텐션은 입력 문장 내의 모든 단어를 **동시에** 처리할 수 있습니다. 이는 모델이 더 빠르고 효율적으로 작동할 수 있게 해줍니다. 
                        예를 들어, 긴 문장도 병렬 처리를 통해 동시에 계산이 가능해 처리 속도가 빨라집니다.

                        2. **긴 문맥 처리 능력**:
                        RNN이나 LSTM은 문장의 앞뒤로 시간이 지남에 따라 정보를 잊어버리는 경향이 있습니다. 반면, 셀프 어텐션은 문장의 **모든 단어를 서로 연결** 하여, 
                        문장이 길어져도 중요한 단어들 간의 관계를 놓치지 않고 잘 처리할 수 있습니다. 두 번째 그림처럼, "카페"와 "거기"라는 단어의 관계를 효과적으로 찾아냅니다.

                        3. **중요한 정보에 집중**:
                        셀프 어텐션은 각 단어가 다른 단어와 얼마나 관련이 있는지를 수치화합니다. 이 수치가 높을수록 더 중요한 관계라고 판단하고, 더 큰 가중치를 부여합니다. 
                        첫 번째 그림에서, "I am a student"라는 문장에서 "I"와 "student" 간의 관계는 1.0으로 표시되었고, 이것이 중요한 정보임을 나타냅니다.

                        4. **멀티-헤드 어텐션(Multi-Head Attention)**:
                        셀프 어텐션은 여러 번의 어텐션을 동시에 적용할 수 있습니다. 이때, 각 어텐션 헤드는 문장의 다른 부분에 집중하게 됩니다. 
                        마지막 그림처럼, 여러 어텐션 헤드가 병렬로 적용되어 더 다양한 관점에서 데이터를 분석할 수 있게 됩니다. 이를 통해 모델은 문장의 의미를 더욱 깊이 있게 이해할 수 있습니다.

                        5. **어휘 불일치 해결**:
                        예를 들어, 셀프 어텐션은 "The animal didn't cross the street because it was too tired"라는 문장에서 "it"이 "animal"을 가리킨다는 것을 정확하게 파악합니다. 
                        이는 셀프 어텐션의 중요한 특징 중 하나로, 단어 간의 문맥을 제대로 파악할 수 있게 해줍니다.

                        ##### 결론:

                        셀프 어텐션은 입력 문장에서 각 단어가 다른 단어들과 얼마나 중요한 관계를 가지고 있는지 계산하여, **문맥을 더 잘 이해**하고, 
                        중요한 정보에 **더 집중**할 수 있도록 도와줍니다.
                        ''')
            with st.expander('**Differences between Self-Attention and Attention**'):
                st.markdown('''
                            **Attention**과 **Self-Attention**의 차이는 **계산의 대상과 방식**에서 비롯됩니다. 두 메커니즘의 차이를 이해하려면, 
                            **Attention**과 **Self-Attention**이 각각 어떤 데이터에 집중하고 계산하는지 살펴보는 것이 중요합니다.

                            ### 1. **Attention (일반 어텐션)**

                            일반 **Attention**은 **두 시퀀스 간의 관계**를 계산하는 메커니즘입니다. 주로 **인코더-디코더 구조** 에서 사용됩니다.

                            - **입력 시퀀스(인코더)** 와 **출력 시퀀스(디코더)** 간의 관계를 계산합니다. 예를 들어, 번역 작업에서, 입력 문장(원문)의 각 단어가 번역될 때, 디코더는 입력 문장의 어떤 단어에 집중할지 결정하는 방식으로 동작합니다.
                            
                            - **쿼리(Query)** 는 디코더의 출력 시퀀스의 단어들에서 오고, **키(Key)** 와 **값(Value)** 는 인코더의 입력 시퀀스에서 옵니다. 즉, 인코더에서 나온 정보와 디코더에서 생성된 단어가 서로 연관성을 계산합니다.

                            ### 2. **Self-Attention**

                            **Self-Attention**은 **자신의 시퀀스 내에서 단어들 간의 관계**를 계산합니다. **단일 시퀀스** 안에서 각 단어가 다른 단어들과 얼마나 연관이 있는지를 계산합니다.

                            - 예를 들어, 하나의 문장 내에서, 특정 단어가 다른 단어들과 얼마나 관련이 있는지를 계산합니다. 번역 작업에서나 문장 생성, 문장 이해에서 중요한 역할을 합니다.
                            
                            - **쿼리(Query)**, **키(Key)**, **값(Value)** 가 모두 **같은 시퀀스** 에서 나옵니다. 즉, 같은 문장의 각 단어가 다른 단어들과 어떤 관계를 맺는지를 계산합니다. 이를 통해 모델은 문장 내에서 서로 연결된 단어들을 더 잘 이해할 수 있게 됩니다.

                            ---

                            ### **차이점 요약**

                            1. **Attention**:
                            - **다른 두 시퀀스** 간의 관계를 계산.
                            - 예: 번역에서 원문(인코더)과 번역된 문장(디코더)의 관계.
                            - **쿼리**는 디코더, **키**와 **값**은 인코더에서 나옴.

                            2. **Self-Attention**:
                            - **같은 시퀀스 내**에서 각 단어 간의 관계를 계산.
                            - 예: 문장 내에서 각 단어가 서로 얼마나 관련이 있는지 계산.
                            - **쿼리**, **키**, **값**이 모두 같은 시퀀스에서 나옴.

                            ''')
            p3 , p3_1 = st.columns([1,1.5])   
            with p3:
                st.image('./img/transformers/transformer5-1.png', width=1000)
            with p3_1:
                st.markdown('''
                                #### Query, Key, Value
                                Query, Key, Value는 Transformer에서 데이터를 처리하고, 중요한 정보를 찾기 위한 세 가지 핵심 요소이며,
                                어텐션 메커니즘은 각 단어가 다른 단어와 얼마나 관련 있는지를 계산해, 중요한 정보에 집중하는 방식을 채택합니다.
                                이 과정을 통해 Transformer는 더 효율적이고 정밀하게 데이터를 처리할 수 있습니다.

                                - **Query** : 내가 지금 집중하고 싶은 정보 또는 질문을 나타냅니다.
                                - **Key** : 입력 데이터의 각 항목이 가진 특성을 표현합니다.
                                - **Value** : 실제 입력 데이터의 정보 자체를 담고 있습니다.

                                1. **단어 "student"를 처리하는 과정**:
                                - 첫 번째 그림에서, 입력 단어 **"student"** 는 세 가지로 변환됩니다: **쿼리(Q)**, **키(K)**, **값(V)**.
                                - 각각의 변환은 다른 가중치 행렬(**WQ**, **WK**, **WV**)을 사용해 계산됩니다. 이 가중치 행렬들은 학습을 통해 결정되며, 입력 단어를 서로 다른 관점에서 바라보도록 변환하는 역할을 합니다.
                                    - **WQ**는 쿼리 행렬을 만들기 위한 가중치 행렬입니다.
                                    - **WK**는 키 행렬을 만들기 위한 가중치 행렬입니다.
                                    - **WV**는 값 행렬을 만들기 위한 가중치 행렬입니다.
                                - 결국, 입력 단어 "student"는 각각 쿼리, 키, 값으로 변환되어, Attention 메커니즘에서 사용됩니다.
                            ''')
            p3_2 , p3_3 = st.columns([1,1.5]) 
            with p3_2:
                st.image('./img/transformers/transformer5-2-1.png')
                with st.expander('**실제 작동 과정**'):
                    st.markdown('''
                            2. **실제 작동 과정**:
                                - 쿼리, 키, 값은 각각 다른 정보를 나타냅니다. 쿼리는 "내가 지금 찾고자 하는 정보"를 의미하고, 키는 "각 데이터가 가진 특성"을 나타냅니다. 
                                마지막으로, 값은 "데이터 자체의 정보"입니다.
                                - 이 과정을 통해, 모델은 입력 데이터 중 어떤 부분이 중요한지 판단하고, 그 정보에 더 집중할 수 있게 됩니다.
                            
                            예를 들어, 문장에서 "학생이 학교에 갔다"라는 문장을 처리할 때, "학생"이라는 단어에 집중하려고 한다면, 
                            어텐션 메커니즘을 통해 이 단어가 다른 단어들과 어떤 관계를 갖고 있는지 계산하고, 중요한 정보를 선택해내는 것입니다.
                            ''')
            with p3_3:
                st.markdown('''
                            #### Attention 계산 과정

                            Attention(어텐션)을 계산하는 공식은 입력 데이터에서 중요한 부분을 찾는 방법을 설명합니다. 
                            어텐션 메커니즘은 데이터의 모든 부분이 서로 얼마나 관련 있는지를 계산하여, 중요한 정보에 집중하는 방식입니다.

                            1. **Attention 공식**:
                                - **Q** 는 **쿼리(Query)**, **K** 는 **키(Key)**, **V** 는 **값(Value)** 입니다.
                                - **쿼리(Q)** 와 **키(K)** 를 내적(dot product)하여 두 벡터의 유사도를 계산합니다. 즉, 각 단어가 다른 단어와 얼마나 연관 있는지를 계산하는 것입니다.
                                - 그 유사도 값을 스케일링(scale)한 후, **softmax** 함수를 적용하여 확률 값으로 변환합니다. 이 과정에서 가장 중요한 정보에 더 높은 가중치를 줍니다.
                                - 마지막으로, **값(Value, V)** 에 이 가중치를 곱하여 최종적인 어텐션 값을 얻습니다.
                            ''')
            p4 , line ,p4_1 ,line2, p4_2 = st.columns([1,0.1,1,0.1,1])
            with p4:
                st.image('./img/transformers/transformer6-1.png')
                st.markdown('''
                            #### Self-Attention의 연관성 매트릭스

                            "I am a student"라는 문장에서 **Self-Attention**이 단어들 간의 연관성을 계산하는 과정을 보여줍니다. 각 단어가 다른 단어와 얼마나 관련이 있는지 수치로 나타내고 있습니다.

                            - **Diagonal values (대각선 값들)**: 각 단어가 자기 자신과의 연관성을 나타내며, 당연히 값이 큽니다. 예를 들어, "I"와 "I"는 1.5, "student"와 "student"는 1.5로 높은 값을 갖고 있습니다.
                            - **Off-diagonal values (대각선 외 값들)**: 각 단어가 다른 단어와 얼마나 관련 있는지 나타냅니다. 
                                예를 들어, "I"와 "student"의 연관성은 1.0으로 상대적으로 높은 편입니다. 이 값을 통해 모델은 문맥적으로 관련된 단어들에 더 집중할 수 있습니다.
                            ''')
            with line:
                st.markdown(
                """
                <div style="border-right: 1px solid  #d2c5d3; height: 700px;"></div>
                """,
                unsafe_allow_html=True)
            with p4_1:
                st.image('./img/transformers/transformer7-2.png')
                st.markdown('''
                            #### 단어 간 상관성

                            Self-Attention이 문장에서 각 단어가 서로 어떻게 연결되는지를 보여줍니다.

                            - 예를 들어, "어제 카페에 갔어요"라는 문장에서 "카페"와 "거기"가 매우 강하게 연결된 것을 볼 수 있습니다. 
                              두 단어는 문맥상 관련이 높으므로 Self-Attention 메커니즘은 이 두 단어 사이의 연관성을 더 크게 계산합니다.
                            - 아래쪽 그림에서는 "카페"가 중요한 단어로 판단되어 다른 단어들과 연결되는 모습을 보여줍니다.

                            이 과정에서 Self-Attention은 단어 간의 상호 관련성을 파악해, 문장의 의미를 더 잘 이해할 수 있게 돕습니다.
                            ''')
            with line2:
                st.markdown(
                """
                <div style="border-right: 1px solid  #d2c5d3; height: 700px;"></div>
                """,
                unsafe_allow_html=True)
            with p4_2:
                st.image('./img/transformers/transformer9-1.png')
                st.markdown('''
                            #### **대명사 참조 문제**

                            Self-Attention이 어떻게 대명사("it")가 가리키는 대상을 파악하는지를 보여줍니다.

                            - 문장 "The animal didn't cross the street because it was too tired"에서, **"it"** 이라는 대명사가 **"animal"** 을 가리키고 있음을 보여줍니다. 
                            Self-Attention 메커니즘은 "it"과 "animal" 사이의 높은 연관성을 계산하여, 모델이 문맥을 더 잘 이해할 수 있도록 도와줍니다.
                            
                            이러한 기능 덕분에 Self-Attention은 문장에서 대명사가 어떤 대상을 가리키는지 더 정확하게 파악할 수 있습니다.
                            ''')
            p4_3 , p4_4 = st.columns([1,1.5])   
            with p4_3:
                st.image('./img/transformers/transformer10-1-1.png')
                with st.expander('**병렬 처리의 중요성**', expanded=True):
                    st.markdown('''
                            - 병렬 처리는 데이터 처리 속도를 획기적으로 향상시킵니다. 특히 매우 긴 문장이나 시퀀스를 처리할 때, 
                            순차적인 연산을 사용하는 RNN/LSTM보다 Self-Attention을 사용한 Transformer 모델이 더 효율적입니다.
                            
                            - 이로 인해 **Transformer** 모델은 RNN/LSTM 모델에 비해 훨씬 빠르게 학습하고, 긴 시퀀스도 처리할 수 있습니다. 
                            이러한 병렬 처리 능력은 대규모 데이터를 다루는 자연어 처리(NLP) 작업에서 매우 중요한 장점입니다.
                                ''')
            with p4_4:
                st.markdown('''

                            ##### **Self-Attention의 병렬 처리 가능성**
                            - **Self-Attention** 은 문장의 모든 단어들을 **동시에** 처리할 수 있습니다. 이 메커니즘에서는 각 단어가 다른 모든 단어와의 상관관계를 한꺼번에 계산하기 때문에, 문장의 길이와 관계없이 한 번에 연산을 수행할 수 있습니다. 이를 통해 **병렬 계산**이 가능해지며, 연산 속도가 크게 향상됩니다.

                                ##### **Scaled Dot-Product Attention (왼쪽)**
                                - **쿼리(Q)** 와 **키(K)** 를 통해 단어 간 유사도를 계산하고, 이를 스케일링하여 안정화한 후, **소프트맥스(SoftMax)** 로 각 단어에 대한 가중치를 부여합니다. 그 가중치를 **값(Value, V)**에 적용하여 최종적인 어텐션 값을 얻습니다.
                                - 이 과정은 모든 단어 쌍에 대해 병렬로 계산됩니다.

                                ##### **4. Multi-Head Attention (오른쪽)**
                                - **Multi-Head Attention**은 여러 개의 Attention을 병렬로 수행합니다. 각각의 Attention 헤드는 입력 데이터의 다른 측면을 분석하며, 여러 시각에서 데이터를 처리합니다.
                                - 마지막에는 각 헤드의 결과를 **Concatenate(연결)** 하여 하나의 통합된 결과로 변환합니다.
                            ''')
        with tab4:
            p5 , p5_1 = st.columns([1,2 ])
            with p5:
                st.image('https://wikidocs.net/images/page/159243/img_original_paper-726x1030.png')
                with st.expander('**Transformer의 특징**', expanded=True):
                    st.markdown('''
                        #### **Transformer의 특징**
                        - **병렬 처리**: RNN이나 LSTM과 달리, Transformer는 문장의 모든 단어를 동시에 처리할 수 있기 때문에 병렬 계산이 가능합니다. 
                            이로 인해 연산 속도가 빠르고 대규모 데이터 처리에 적합합니다.
                        
                        - **Self-Attention**: 각 단어가 문장 내 다른 단어들과의 관계를 계산하여 중요한 정보에 더 집중할 수 있도록 도와줍니다. 이를 통해 문장의 문맥을 더 잘 파악할 수 있습니다.

                        - **Multi-Head Attention**: 여러 개의 Attention 헤드를 사용하여 다양한 시각에서 단어 간의 관계를 파악합니다. 각 헤드는 서로 다른 방식으로 문장을 해석하며, 이를 통해 더 정교한 정보를 얻을 수 있습니다.

                                ''')
            with p5_1:
                st.write('Transformer는 **인코더-디코더** 구조로 이루어져 있으며, 각 단계에서 **Multi-Head Attention**과 **Feed Forward Layer**를 활용하여 데이터를 처리합니다. ')
                p5_1_1 , p5_1_2 = st.columns([1,1])
                with p5_1_1:
                    st.markdown('''
                        #### **1. 인코더(Encoder) 구조 (왼쪽)**

                        - **Input Embedding**: 입력된 단어들을 고정된 크기의 벡터로 변환하는 단계입니다. 각 단어는 고유한 벡터로 표현되며, 이 벡터는 모델이 단어 간의 관계를 학습하는 데 사용됩니다.

                        - **Positional Encoding**: Transformer는 순차적으로 데이터를 처리하지 않기 때문에, 단어의 순서 정보를 추가해주는 **포지셔널 인코딩(Positional Encoding)**이 필요합니다. 이를 통해 모델은 문장에서 각 단어의 위치를 알 수 있습니다.

                        - **Multi-Head Attention**: 입력된 각 단어와 다른 단어 간의 상호 연관성을 동시에 계산합니다. 여러 개의 Attention 헤드가 병렬로 동작하여, 단어 간의 관계를 다양한 시각에서 분석합니다.

                        - **Add & Norm**: Residual connection(잔여 연결)을 통해 이전 단계의 입력과 Attention 결과를 더한 후, **정규화(Normalization)** 과정을 거칩니다. 이는 네트워크 학습의 안정성을 높이는 데 도움을 줍니다.

                        - **Feed Forward**: 각 단어 벡터에 독립적으로 두 개의 완전 연결 층(Fully Connected Layer)을 적용하여 복잡한 패턴을 학습합니다.

                        - **Nx 반복**: 위 과정을 여러 번 반복합니다. Transformer에서는 일반적으로 인코더 블록을 여러 층으로 쌓아 학습의 깊이를 더합니다.
                        ''')
                with p5_1_2:
                    st.markdown('''


                        #### **2. 디코더(Decoder) 구조 (오른쪽)**

                        - **Output Embedding**: 디코더도 입력된 데이터(출력할 단어)를 고정된 크기의 벡터로 변환합니다. 그리고 인코더와 마찬가지로 **Positional Encoding**을 추가하여 단어의 순서를 이해할 수 있게 합니다.

                        - **Masked Multi-Head Attention**: 디코더는 **Masked Attention**을 사용하여 출력 단어를 한 번에 하나씩 생성합니다. 즉, 미래의 단어를 참조하지 않도록 마스킹(masking)을 적용하여, 이전에 생성된 단어만을 이용해 다음 단어를 예측합니다.

                        - **Multi-Head Attention**: 디코더는 인코더에서 얻은 정보를 활용하기 위해 인코더의 출력과 상호작용하는 **Multi-Head Attention**을 추가로 사용합니다. 이 단계에서 디코더는 입력 데이터와의 관계를 학습합니다.

                        - **Feed Forward**: 인코더와 동일하게, 디코더에서도 각 단어에 독립적으로 완전 연결 층을 적용하여 패턴을 학습합니다.

                        - **Add & Norm**: 마찬가지로 Residual connection을 통해 정규화 과정을 거칩니다.

                        - **Nx 반복**: 이 과정도 여러 층으로 반복되며, 디코더는 점점 더 정교한 출력을 생성할 수 있게 됩니다.

                        ''')
                st.markdown('''
                            ### **3. 최종 출력**
                            - **Linear & Softmax**: 마지막으로, 출력 벡터는 **Linear 층**을 거쳐 차원을 변환하고, **Softmax**를 통해 확률 분포로 변환됩니다. 
                            이 확률 분포에서 가장 높은 값을 가진 단어가 최종 출력으로 선택됩니다.
                            ''')
            st.markdown('---')
            p5_2,line,p5_3 = st.columns([1,0.1,1])   
            with p5_2:
                st.image('https://wikidocs.net/images/page/31379/transformer6_final.PNG')
                st.markdown('''
                        ### **1. Positional Encoding (위치 인코딩)**

                        - **Positional Encoding**은 이 단어들의 **위치 정보**를 나타냅니다. 예를 들어, 문장에서 "I"는 첫 번째 단어이고, "am"은 두 번째, "student"는 네 번째 단어라는 정보를 모델에 추가로 제공하는 역할을 합니다.
                        - Positional Encoding은 각 단어의 벡터에 더해지며, 이를 통해 모델은 단어의 **위치**와 **순서**를 파악할 수 있습니다.
                            
                        ### **2. Embedding Vector (임베딩 벡터)**

                        - **Embedding Vector**는 각 단어를 고정된 크기의 벡터로 변환한 것입니다. 예를 들어, "I", "am", "a", "student"와 같은 단어들은 각각 특정 벡터로 변환됩니다.
                        - 이 벡터는 각 단어의 의미적 정보를 포함하며, 단어 간의 유사성을 나타냅니다. 하지만 이 벡터만으로는 단어의 **순서 정보**가 부족합니다. 예를 들어, "I am a student"과 "student a am I"라는 문장이 같은 의미로 처리될 위험이 있습니다.

                        ### **3. 벡터의 합**

                        - **Embedding Vector**와 **Positional Encoding**은 단순히 더해져 결합됩니다. 
                            각 단어 벡터는 단어의 의미 정보를 담고 있고, Positional Encoding은 그 단어가 문장에서 어느 위치에 있는지 알려줍니다.
                        - 이 두 정보를 결합함으로써 Transformer 모델은 문장 내에서 각 단어의 의미뿐만 아니라 그 단어의 위치도 동시에 고려하게 됩니다.
                        ''')
            with line:
                st.markdown(
                """
                <div style="border-right: 1px solid  #d2c5d3; height: 950px;"></div>
                """,
                unsafe_allow_html=True)
            with p5_3:
                st.image('./img/transformers/transforner13-2.png')
                st.markdown('''
                            **Multi-Head Attention**은 Transformer 모델의 중요한 요소로, 문장을 여러 시각에서 분석할 수 있게 해줍니다. 각각의 **어텐션 헤드**가 다른 부분에 집중하여, 다양한 정보를 동시에 파악하는 능력을 제공합니다.

                            1. **문장 타임에 집중하는 어텐션 (첫 번째 줄)**:
                            - 이 어텐션 헤드는 문장의 **문법적 구조**에 집중합니다.
                            - "Which"와 "do"와 같은 문장 구성 요소(의문사, 조동사)와 같은 **문법적 역할**을 가진 단어에 집중합니다.
                            - 문장의 형태와 구조를 파악하는 데 중요한 정보입니다.

                            2. **명사에 집중하는 어텐션 (두 번째 줄)**:
                            - 이 어텐션 헤드는 문장 내의 **명사**에 집중하고 있습니다.
                            - "you", "coffee", "tea"와 같은 명사들이 강조됩니다.
                            - 이는 문장에서 **대상**을 파악하거나 대상을 비교할 때 중요한 역할을 합니다.

                            3. **관계에 집중하는 어텐션 (세 번째 줄)**:
                            - 이 헤드는 문장 내에서 **동사**나 **관계**에 집중합니다.
                            - "like"와 같은 동사가 강조되며, 문장의 주된 동작이나 행위에 중점을 둡니다.
                            - **동사**와 **대상** 간의 관계를 분석하는 데 중요합니다.

                            4. **감정에 집중하는 어텐션 (네 번째 줄)**:
                            - 이 헤드는 문장에서 **감정**이나 **선호**를 나타내는 단어에 집중합니다.
                            - "like"와 "better" 같은 감정과 선호를 표현하는 단어들이 강조됩니다.
                            - 이 어텐션은 문장의 **의도**나 **감정**을 파악하는 데 도움을 줍니다.
                            ''')
            st.markdown('---')
            p5_4,line,p5_5 = st.columns([1,0.1,1])
            with p5_4:
                st.image('https://wikidocs.net/images/page/31379/%EB%A3%A9%EC%96%B4%ED%97%A4%EB%93%9C%EB%A7%88%EC%8A%A4%ED%81%AC.PNG',width=300)
                st.markdown('''
                            출력 데이터를 생성할 때 모델이 **미래의 정보**를 미리 참조하지 않도록 방지하는 역할을 합니다 디코더 단계에서 이루어집니다.

                            #### **Masked Multi-Head Attention의 역할**

                            1. **미래 정보 가리기**:
                            - **디코더**에서는 문장을 생성할 때, 모델이 출력 시퀀스의 **미래 단어**를 미리 참조하면 안 됩니다. 즉, 아직 생성되지 않은 단어를 참조하여 답을 예측하는 상황을 방지해야 합니다.
                            - 이를 위해 **Masking(마스킹)** 기법을 사용합니다. **마스킹**을 통해, 모델이 현재 단어까지만 참조하도록 제한하고, 미래의 단어는 **가려진 상태**로 남겨둡니다.

                            2. **Attention Score Matrix**:
                            - 그림에 보이는 **Attention Score Matrix**는 문장에서 단어들 간의 상관관계를 시각적으로 보여줍니다.
                            - 예시로, **프랑스어 문장** "<sos> je suis étudiant"의 각 단어 사이의 관계를 나타내고 있습니다. 마스킹이 적용된 상태에서, 현재 단어까지만 상관관계를 계산하며, 미래의 단어는 참조되지 않습니다.
                            - 검은색 셀은 상관관계가 높은 단어들을, 흰색 셀은 상관관계가 낮은 단어들을 나타냅니다.

                            #### **왜 디코더에서만 사용되는가?**
                            - **인코더**는 입력 데이터를 모두 참조하여 전체 정보를 처리할 수 있지만, **디코더**는 출력 데이터를 **순차적으로** 생성하기 때문에 **미래 단어**를 참조하면 안 됩니다.
                            - 이를 해결하기 위해 Masked Attention을 사용하여, **출력 단어가 아직 생성되지 않았을 때는 가려져 있도록**(마스킹) 처리합니다.

                            #### **병렬 처리와 Masked Attention**
                            - Transformer는 문장의 모든 단어를 **동시에** 처리하기 때문에 병렬 연산이 가능합니다. 하지만 디코더에서는 미래 단어를 참조하면 안 되므로, **마스킹을 통해 제한**을 두는 것이 필수적입니다.
                            - **Masked Multi-Head Attention**은 이러한 문제를 해결하면서도, 병렬 처리의 이점을 유지할 수 있게 해줍니다.


                            ''')
            with line:
                st.markdown(
                """
                <div style="border-right: 1px solid  #d2c5d3; height: 1200px;"></div>
                """,
                unsafe_allow_html=True)
            with p5_5:
                st.image('./img/transformers/transformer15-2.png')
                st.markdown('''
                            **Residual Connection**은 깊은 신경망에서 발생할 수 있는 **기울기 소실(vanishing gradient)** 문제를 해결하기 위한 기법으로, 
                            **Transformer** 모델에서 중요한 역할을 합니다. 이 기법을 사용하면 신경망이 더 깊어지더라도 학습이 잘 이루어질 수 있습니다.

                            #### **1. Residual Connection (잔차 연결)**

                            - **입력 \( x \)** 는 **Weight Layer(가중치 층)** 을 통과하면서 변환됩니다. 변환된 결과는 **\( F(x) \)** 로 나타냅니다.
                            - 그러나, 단순히 변환된 결과만 사용하는 것이 아니라, **입력 \( x \)** 를 **잔차(Residual)** 로 더해줍니다. 
                            이를 통해 신경망이 깊어져도 입력 값이 유지되는 특성이 생깁니다.
                            
                            즉, 최종 출력은 **\( F(x) + x \)** 로 계산됩니다. 이 방식은 모델이 입력 값을 보존하면서도 추가적인 학습을 할 수 있게 하며, 기울기 소실을 막아 학습 안정성을 높입니다.

                            #### **2. Add & Norm (정규화와 더하기)**

                            - **Add**: 먼저, 잔차 연결을 사용하여 **변환된 값 \( F(x) \)** 와 **입력 값 \( x \)** 를 더해줍니다.
                            - **Norm (정규화)**: 그 다음, **정규화(Normalization)** 과정을 통해 데이터의 분포를 조정합니다. 
                            정규화를 통해 학습 속도가 빨라지고, 과적합(overfitting)을 방지하는 효과가 있습니다.

                            이 두 과정을 거치면서 모델은 더 깊은 층을 학습하면서도, 입력의 중요한 정보가 손실되지 않도록 할 수 있습니다.

                            #### **3. Residual Block의 필요성**

                            - **기울기 소실 문제 해결**: 신경망이 깊어질수록 역전파 과정에서 기울기가 점점 작아져, 학습이 어려워지는 **기울기 소실** 문제가 발생할 수 있습니다. 
                            Residual Connection은 이러한 문제를 해결하여, 더 깊은 네트워크에서도 학습이 가능하게 만듭니다.
                            - **정규화**: 데이터 분포를 조정하여 학습을 빠르고 안정적으로 만들기 위해 **정규화**가 필수적입니다. 
                            이를 통해 네트워크는 더 효율적으로 학습할 수 있습니다.
                            ''')
            st.markdown('---')
            p5_6,p5_7 = st.columns([1,1])
            with p5_6:
                st.image('./img/transformers/transformer16-2.png',width=600)
            with p5_7:
                with st.expander('수식 설명'):
                    st.markdown('''
                                \[
                                X' = MLP_{x4}(ReLU(MLP_{x4}(X)))
                                \]
                                - **\( MLP_{x4} \)**: 첫 번째 층에서 입력 벡터의 차원을 4배로 확장한 후, 두 번째 층에서 다시 원래 차원으로 축소합니다.
                                - **ReLU**: 중간에 **ReLU 활성화 함수**를 사용하여 비선형성을 도입합니다.
                                ''')
            st.markdown('''
                    **Feed Forward**는 Transformer 모델의 각 레이어에서 **Attention 결과를 취합하고 처리**하는 역할을 합니다.
                    ''')
            pp1,pp2,pp3 = st.columns([1,1,1])
            with pp1:
                st.markdown('''
                    #### **1. Feed Forward 네트워크의 역할**
                    - **Attention 메커니즘**은 각 단어의 상관관계를 계산하여 중요한 정보를 추출하지만, 그것만으로는 충분하지 않습니다. 
                            Attention 결과를 바탕으로, **더 복잡한 패턴**을 학습하기 위해 추가적인 처리가 필요합니다.
                    - **Feed Forward 네트워크**는 이러한 Attention 결과를 취합하여, **비선형적인 변환**을 수행하고, 더 복잡한 정보를 학습할 수 있도록 돕습니다.
                            ''')
            with pp2:
                st.markdown('''
                    #### **2. Feed Forward의 구조**
                    - 입력 벡터 **\( x \)** 는 **Attention**의 결과물로, 이 벡터는 **MLP(다층 퍼셉트론)** 구조로 전달됩니다.
                    - **MLP**는 두 개의 완전 연결 층(Fully Connected Layer)으로 구성되어 있으며, 중간에 **비선형 활성화 함수(ReLU)**를 적용합니다.
                    - 첫 번째 완전 연결 층은 입력 벡터의 차원을 늘려서 더 큰 공간에서 변환을 수행합니다. 이는 더 복잡한 패턴을 학습할 수 있게 해줍니다.
                    - 그다음 ReLU 활성화 함수를 적용해 **비선형성**을 도입합니다. 이를 통해 데이터의 복잡한 관계를 모델링할 수 있습니다.
                    - 두 번째 완전 연결 층은 다시 원래 차원으로 변환합니다.
                            ''')      
            with pp3:
                st.markdown('''
                    #### **3. Feed Forward의 기능**
                    - **정보 취합**: Feed Forward 네트워크는 **Attention**을 통해 추출된 정보를 바탕으로, 더 복잡한 패턴을 학습하고, 각 단어 간의 관계를 더 정교하게 분석할 수 있습니다.
                    - **비선형 변환**: ReLU 같은 비선형 활성화 함수를 사용해 데이터에 비선형성을 추가함으로써, 단순한 선형 관계만을 학습하는 것이 아니라 더 복잡한 관계를 학습할 수 있습니다.
                            ''')