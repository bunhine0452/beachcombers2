import streamlit as st
from css import hard_type_css
import os

from funcs import linegaro
def transformer_info():
    a,b,c,d = st.columns([0.05, 0.1, 0.01 ,1])
    with a:
        st.write('')
        st.image('https://kli.korean.go.kr/images/logo_kor_2.png')
    with b:
        st.write('')
        st.write('')
        st.image('https://kli.korean.go.kr/images/logo_kor_header.png', width=400)
    with d:
        st.title('Transformer 에 대하여')
    st.markdown('#')
    with st.expander('Before we go'):
        st.markdown('''
                ### **Transformer?**
                
                Transformer는 자연어 처리(NLP)에서 가장 중요한 딥러닝 모델 중 하나로, 특히 번역, 문장 생성, 텍스트 분류 같은 작업에서 뛰어난 성능을 보입니다. 
                2017년에 "Attention is All You Need"라는 논문에서 처음 제안되었으며, 이후 많은 최신 언어 모델들이 Transformer 구조를 기반으로 만들어졌습니다.

                #### **핵심 개념**
                - **Attention 메커니즘**: Transformer는 "어텐션"이라는 메커니즘을 사용하여 문장 내에서 중요한 단어에 집중합니다. 예를 들어, 긴 문장에서 특정 단어가 다른 단어들과 어떻게 연관되어 있는지를 어텐션으로 파악합니다.
                - **병렬 처리**: 기존 순차적 모델(RNN, LSTM)과 달리, Transformer는 병렬 처리가 가능해 훈련 속도가 빠르고 더 긴 문장도 효과적으로 처리할 수 있습니다.
                - **Encoder-Decoder 구조**: Transformer는 인코더와 디코더로 구성됩니다. 인코더는 입력 문장을 처리하고, 디코더는 그 정보를 바탕으로 새로운 문장을 생성합니다. 번역 작업을 예로 들면, 인코더는 한국어 문장을 이해하고, 디코더는 영어 문장을 생성하는 방식입니다.

                #### **활용 예**
                GPT, BERT, T5와 같은 유명한 언어 모델들이 모두 Transformer를 기반으로 만들어졌습니다. 이 모델들은 챗봇, 자동 번역, 글 요약, 문장 생성 등 다양한 작업에 사용되고 있습니다.

                간단히 말해, Transformer는 언어 모델이 문맥을 이해하고, 다양한 언어 작업을 수행할 수 있게 하는 매우 강력한 기술입니다.
                ''')
    st.write('탭으로 이동')
    tab1, tab2 ,tab3, tab4= st.tabs(['RNN', 'Seq2Seq','Attention:Q,K,V 그리고 계산과정','Self-Attention'])
    with tab1:
        st.image('https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg')
        st.markdown('''RNN은 컴퓨터가 순차적으로 데이터를 처리하는 방식 중 하나로, 주로 시간에 따라 변화하는 데이터를 다룰 때 유용합니다. 
                예를 들어, 문장의 단어를 차례로 처리하거나, 음성 데이터를 시간 순서대로 분석할 때 사용됩니다.''')
        p1 , p1_1 = st.columns([1,1])
        with p1:
            st.markdown('''
                        ### RNN 구조

                        1. **왼쪽의 기본 구조**:
                        - **h**는 **은닉 상태(hidden state)** 라고 부르는데, 이 값은 이전에 처리한 정보(데이터)를 기억하고 저장하는 역할을 합니다. RNN이 데이터를 처리할 때, 이전에 봤던 데이터를 계속 기억하면서 새로운 데이터를 이해하려고 합니다.
                        - **x**는 **입력 값**입니다. 예를 들어, 입력값이 문장의 단어라면, RNN은 각 단어를 순서대로 처리하게 됩니다.
                        - **o**는 **출력 값**입니다. RNN이 최종적으로 만들어내는 결과물입니다. 예를 들어, 텍스트를 입력받고 번역을 할 때 번역된 문장이 출력값이 됩니다.
                        - **w**와 **v**, **U**는 계산에 사용되는 값(가중치)들인데, 데이터가 어떻게 변하는지 결정하는 역할을 합니다.

                        2. **오른쪽 펼쳐진 구조 (Unfold)**:
                        - 그림의 오른쪽은 RNN이 여러 개의 입력을 시간 순서대로 처리하는 모습을 나타냅니다. 
                        - **h(t-1), h(t), h(t+1)** 은 각각 다른 시간에 처리된 데이터의 은닉 상태입니다. 이 값들은 순서대로 연결되며, 이전에 봤던 데이터의 정보가 다음 데이터 처리에 영향을 줍니다.
                        - **x(t-1), x(t), x(t+1)** 은 시간에 따라 들어오는 입력 값들입니다. 예를 들어, 첫 번째 단어가 **x(t-1)**, 두 번째 단어가 **x(t)**, 세 번째 단어가 **x(t+1)** 라고 할 수 있습니다.
                        - 이 값들이 연속적으로 연결되면서 RNN은 앞에서 처리한 데이터를 기억하며, 문장이나 데이터의 전체 흐름을 파악하려고 합니다.
                        ''')
        with p1_1:
            st.markdown('''    
                        ### 쉽게 이해하는 예
                        예를 들어, RNN이 "나는 사과를 먹었다"라는 문장을 처리한다고 해봅시다. 
                        1. 처음에 "**나는**"을 입력받으면, 이 단어를 기억하며, RNN의 은닉 상태는 "**나는**"이라는 정보를 저장합니다.
                        2. 다음으로 "**사과를**"을 입력받으면, RNN은 "**나는**"을 기억한 상태에서 "**사과를**"이라는 단어를 처리합니다.
                        3. 마지막으로 "**먹었다**"를 입력받으면, 앞서 기억한 "**나는 사과를**" 정보를 바탕으로 "**먹었다**"라는 단어를 이해하고 전체 문장의 의미를 파악하게 됩니다.

                        이렇게 RNN은 데이터를 순서대로 처리하며, 앞에서 받은 정보를 잊지 않고 기억하면서 새로운 데이터를 처리하는 것이 특징입니다. 이 덕분에 문장이나 음악처럼 시간에 따라 변화하는 데이터를 효과적으로 분석할 수 있습니다.
                        
                        ### RNN의 한계
                        RNN의 한계는 데이터가 길어질수록(예: 긴 문장) 앞부분의 정보를 점점 잊어버리기 쉽다는 점인데, 이를 개선하기 위해 더 나은 모델들이 나왔습니다. Transformer 모델도 이런 문제를 해결한 대표적인 기술 중 하나입니다.
                        ''')
    with tab2:
        st.image('https://incredible.ai/assets/images/seq2seq-seq2seq_ts.png', width=4000)
        st.markdown('''
                    Seq2Seq 모델은 하나의 시퀀스(입력 데이터)를 받아 또 다른 시퀀스(출력 데이터)를 생성하는 모델입니다. 이 모델은 인코더와 디코더로 구성되며, 
                    입력 데이터를 요약하고 그 요약된 정보를 바탕으로 출력을 생성하는 구조를 가지고 있습니다.
                    Seq2Seq는 주로 자연어 처리에서 두 개의 다른 시퀀스를 입력과 출력으로 변환하는 데 사용되며,
                    번역, 챗봇, 요약 등의 작업에 매우 유용합니다. 이 모델은 **인코더(Encoder)** 와 **디코더(Decoder)** 라는 두 가지 주요 부분으로 구성되어 있습니다.
                    ''')
        p2 , p2_1 = st.columns([1,1])
        with p2:
            st.markdown('''
                        ### 설명

                        1. **인코더(Encoder)**
                        - 왼쪽의 초록색 박스들로 구성된 부분입니다. 각 박스는 **GRU(Gated Recurrent Unit)** 라는 특별한 형태의 RNN 셀을 나타냅니다. 
                        GRU는 순차 데이터를 처리하는 데 사용되며, 데이터의 과거 정보를 기억하고, 중요한 정보를 남기며, 불필요한 정보는 잊어버리는 역할을 합니다.
                        - **x₁, x₂, ..., xₙ**은 입력 데이터(과거 데이터)를 나타냅니다. 이 입력 데이터는 순차적으로 GRU를 통과하면서, **인코더 상태(Encoder State)** 로 변환됩니다. 
                        이 상태는 인코더가 입력 데이터를 요약한 값으로, 디코더가 사용할 중요한 정보입니다.

                        2. **디코더(Decoder)**
                        - 오른쪽의 보라색 박스들로 구성된 부분입니다. 디코더 역시 GRU 셀로 구성되어 있으며, 인코더에서 전달받은 **인코더 상태**를 기반으로 새로운 시퀀스를 생성합니다.
                        - **y₁, y₂, ..., yₙ**은 출력 데이터(예측값)을 나타냅니다. 디코더는 인코더 상태와 자신의 예측값을 사용해 다음 값을 예측하는 방식으로 출력을 만듭니다.
                        - 예를 들어, 번역 작업에서는 입력 데이터가 한국어 문장이라면, 출력 데이터는 영어 문장이 될 수 있습니다.
                        ''')
        with p2_1:
            st.markdown('''
                        ### 작동 방식

                        1. **인코더**는 입력 시퀀스(문장, 텍스트, 시계열 데이터 등)를 받아 순차적으로 처리한 후, 그 데이터를 하나의 벡터(인코더 상태)로 요약합니다. 이 과정에서 입력 데이터의 패턴과 구조를 이해하게 됩니다.
                        2. **디코더**는 인코더로부터 받은 상태 벡터를 바탕으로 새로운 시퀀스를 만듭니다. 이때, 디코더는 이전에 예측한 값을 사용해 다음 값을 계속해서 예측합니다.

                        ### 예시:
                        Seq2Seq 모델은 번역 작업에서 자주 사용됩니다.
                        - **인코더**는 한국어 문장을 읽고, 그 정보를 요약합니다.
                        - **디코더**는 요약된 정보를 바탕으로 영어 문장을 하나씩 만들어냅니다. "나는 학교에 간다"라는 문장을 입력받았다면, 디코더는 "I go to school"이라는 문장을 차례차례 생성합니다.

                        ### GRU
                        GRU는 RNN의 변형 중 하나로, 데이터의 중요한 부분을 기억하고, 덜 중요한 부분은 잊는 기능을 자동으로 조절하는 장치입니다. RNN이 긴 데이터를 처리하면서 정보를 잊는 문제를 해결하기 위해 고안된 셀입니다. GRU는 **LSTM**과 비슷하지만, 구조가 더 단순해 빠르게 처리할 수 있습니다.

                        ''')
        st.markdown('''
                    
                    ### Seq2Seq의 한계와 Attention의 도입 이유

                    1. **Seq2Seq의 문제점**:
                    - Seq2Seq에서 인코더는 입력 시퀀스를 **하나의 고정된 벡터**로 압축합니다. 이 벡터는 입력 시퀀스의 모든 정보를 담고 있어야 하며, 디코더는 이 벡터만을 참고해 출력을 생성합니다.
                    - 문제는 **긴 문장** 을 처리할 때 발생합니다. 모든 정보를 하나의 벡터에 담기 때문에, 긴 문장의 경우 앞부분의 정보가 뒤로 갈수록 소실될 수 있습니다. 
                      이로 인해, 출력 문장 생성 시 중요한 정보가 잘 전달되지 못할 수 있습니다.

                    2. **Attention의 도입**:
                    - **Attention** 은 Seq2Seq의 이 한계를 극복하기 위해 도입되었습니다. 인코더가 입력 시퀀스의 각 단어를 처리할 때, 단순히 하나의 벡터로 요약하지 않고, **모든 단어 정보를 별도로 유지** 합니다.
                    - 디코더는 출력 시퀀스를 생성할 때, 인코더의 각 단어와의 **연관성을 계산**합니다. 이때, **Attention 메커니즘** 은 디코더가 각 입력 단어가 얼마나 중요한지를 평가할 수 있게 도와줍니다.
                    
                    ### Attention은 어떻게 계산을 보완하는가?

                    - Seq2Seq는 입력 전체를 한 번에 압축하는 방식이라면, **Attention** 은 디코더가 출력의 각 단계에서 **모든 입력 단어에 주목** 할 수 있게 해줍니다.
                    - Attention은 입력 시퀀스의 모든 단어가 **출력 시퀀스의 각 단어에 대해 얼마나 중요한지 계산** 하여, **중요한 단어에 더 큰 가중치를 부여** 합니다. 
                    - 이 과정은 Attention의 **쿼리(Query)**, **키(Key)**, **값(Value)** 를 통해 이루어지며, 단어 간의 상관성을 수학적으로 계산하여 더 나은 출력을 생성할 수 있게 만듭니다.

                    ### 정리

                    Seq2Seq는 기본적으로 데이터를 순차적으로 처리하는 강력한 모델이지만, **긴 문장**에서 정보 손실이 발생할 수 있는 단점이 있었습니다. 
                    **Attention 메커니즘**은 이러한 한계를 극복하기 위해 **디코더가 입력 시퀀스 전체를 동적으로 참조**할 수 있게 만들었고, 그 결과 중요한 정보를 놓치지 않고 잘 처리할 수 있게 되었습니다. 
                    ''')
        with tab3:
            p3 , p3_1 = st.columns([1,1.5])   
            with p3:
                st.image('https://images.velog.io/images/cha-suyeon/post/26e3183e-53b1-4ad0-a6ff-dc5bd5a4591d/image.png', width=1000)
            with p3_1:
                st.markdown('''
                                ### Query, Key, Value
                                Query, Key, Value는 Transformer에서 데이터를 처리하고, 중요한 정보를 찾기 위한 세 가지 핵심 요소이며,
                                어텐션 메커니즘은 각 단어가 다른 단어와 얼마나 관련 있는지를 계산해, 중요한 정보에 집중하는 방식을 채택합니다.
                                이 과정을 통해 Transformer는 더 효율적이고 정밀하게 데이터를 처리할 수 있습니다.

                                - **Query** : 내가 지금 집중하고 싶은 정보 또는 질문을 나타냅니다.
                                - **Key** : 입력 데이터의 각 항목이 가진 특성을 표현합니다.
                                - **Value** : 실제 입력 데이터의 정보 자체를 담고 있습니다.

                                1. **단어 "student"를 처리하는 과정**:
                                - 첫 번째 그림에서, 입력 단어 **"student"** 는 세 가지로 변환됩니다: **쿼리(Q)**, **키(K)**, **값(V)**.
                                - 각각의 변환은 다른 가중치 행렬(**WQ**, **WK**, **WV**)을 사용해 계산됩니다. 이 가중치 행렬들은 학습을 통해 결정되며, 입력 단어를 서로 다른 관점에서 바라보도록 변환하는 역할을 합니다.
                                    - **WQ**는 쿼리 행렬을 만들기 위한 가중치 행렬입니다.
                                    - **WK**는 키 행렬을 만들기 위한 가중치 행렬입니다.
                                    - **WV**는 값 행렬을 만들기 위한 가중치 행렬입니다.
                                - 결국, 입력 단어 "student"는 각각 쿼리, 키, 값으로 변환되어, Attention 메커니즘에서 사용됩니다.
                            ''')
            p3_2 , p3_3 = st.columns([1,1.5]) 
            with p3_2:
                st.image('https://velog.velcdn.com/images%2Fcha-suyeon%2Fpost%2Fba830026-6d8f-4e77-b288-f75dd3a51457%2Fimage.png')
                with st.expander('**그래서 Attention이 뭐죠?**'):
                    st.markdown('''
                                **어텐션(Attention)** 은 모델이 입력 데이터 중에서 가장 중요한 부분을 선택하고 집중하는 **메커니즘** 입니다. 
                                즉, 모든 입력 데이터를 동일하게 처리하지 않고, 특정 단어가 다른 단어들과 얼마나 관련이 있는지 계산한 후, 
                                중요한 정보에 더 많은 가중치를 부여하여 처리하는 방식입니다. 이를 통해 모델은 긴 문장이나 복잡한 데이터를 더 효과적으로 이해하고, 
                                필요한 정보에 더 잘 집중할 수 있게 됩니다.
                                ''')
            with p3_3:
                st.markdown('''
                            ### Attention 계산 과정

                            Attention(어텐션)을 계산하는 공식은 입력 데이터에서 중요한 부분을 찾는 방법을 설명합니다. 
                            어텐션 메커니즘은 데이터의 모든 부분이 서로 얼마나 관련 있는지를 계산하여, 중요한 정보에 집중하는 방식입니다.

                            1. **Attention 공식**:
                                - **Q** 는 **쿼리(Query)**, **K** 는 **키(Key)**, **V** 는 **값(Value)** 입니다.
                                - **쿼리(Q)** 와 **키(K)** 를 내적(dot product)하여 두 벡터의 유사도를 계산합니다. 즉, 각 단어가 다른 단어와 얼마나 연관 있는지를 계산하는 것입니다.
                                - 그 유사도 값을 스케일링(scale)한 후, **softmax** 함수를 적용하여 확률 값으로 변환합니다. 이 과정에서 가장 중요한 정보에 더 높은 가중치를 줍니다.
                                - 마지막으로, **값(Value, V)** 에 이 가중치를 곱하여 최종적인 어텐션 값을 얻습니다.

                            2. **실제 작동 과정**:
                                - 쿼리, 키, 값은 각각 다른 정보를 나타냅니다. 쿼리는 "내가 지금 찾고자 하는 정보"를 의미하고, 키는 "각 데이터가 가진 특성"을 나타냅니다. 
                                마지막으로, 값은 "데이터 자체의 정보"입니다.
                                - 이 과정을 통해, 모델은 입력 데이터 중 어떤 부분이 중요한지 판단하고, 그 정보에 더 집중할 수 있게 됩니다.
                            
                            예를 들어, 문장에서 "학생이 학교에 갔다"라는 문장을 처리할 때, "학생"이라는 단어에 집중하려고 한다면, 
                            어텐션 메커니즘을 통해 이 단어가 다른 단어들과 어떤 관계를 갖고 있는지 계산하고, 중요한 정보를 선택해내는 것입니다.
                            ''')
        with tab4:
            st.markdown('''
                        ### 셀프 어텐션(Self-Attention)이란?

                        셀프 어텐션은 문장의 각 단어가 다른 단어들과 얼마나 관련 있는지를 계산하여, **중요한 단어에 더 집중**할 수 있도록 하는 메커니즘입니다. 이 메커니즘을 통해 모델은 문장 내에서 단어 간의 상호 관계를 효과적으로 파악할 수 있습니다.
                        ''')
            with st.expander('**셀프 어텐션의 주요 장점**'):
                st.markdown('''

                        1. **병렬 처리 가능**:
                        RNN과 달리, 셀프 어텐션은 입력 문장 내의 모든 단어를 **동시에** 처리할 수 있습니다. 이는 모델이 더 빠르고 효율적으로 작동할 수 있게 해줍니다. 
                        예를 들어, 긴 문장도 병렬 처리를 통해 동시에 계산이 가능해 처리 속도가 빨라집니다.

                        2. **긴 문맥 처리 능력**:
                        RNN이나 LSTM은 문장의 앞뒤로 시간이 지남에 따라 정보를 잊어버리는 경향이 있습니다. 반면, 셀프 어텐션은 문장의 **모든 단어를 서로 연결** 하여, 
                        문장이 길어져도 중요한 단어들 간의 관계를 놓치지 않고 잘 처리할 수 있습니다. 두 번째 그림처럼, "카페"와 "거기"라는 단어의 관계를 효과적으로 찾아냅니다.

                        3. **중요한 정보에 집중**:
                        셀프 어텐션은 각 단어가 다른 단어와 얼마나 관련이 있는지를 수치화합니다. 이 수치가 높을수록 더 중요한 관계라고 판단하고, 더 큰 가중치를 부여합니다. 
                        첫 번째 그림에서, "I am a student"라는 문장에서 "I"와 "student" 간의 관계는 1.0으로 표시되었고, 이것이 중요한 정보임을 나타냅니다.

                        4. **멀티-헤드 어텐션(Multi-Head Attention)**:
                        셀프 어텐션은 여러 번의 어텐션을 동시에 적용할 수 있습니다. 이때, 각 어텐션 헤드는 문장의 다른 부분에 집중하게 됩니다. 
                        마지막 그림처럼, 여러 어텐션 헤드가 병렬로 적용되어 더 다양한 관점에서 데이터를 분석할 수 있게 됩니다. 이를 통해 모델은 문장의 의미를 더욱 깊이 있게 이해할 수 있습니다.

                        5. **어휘 불일치 해결**:
                        예를 들어, 셀프 어텐션은 "The animal didn't cross the street because it was too tired"라는 문장에서 "it"이 "animal"을 가리킨다는 것을 정확하게 파악합니다. 
                        이는 셀프 어텐션의 중요한 특징 중 하나로, 단어 간의 문맥을 제대로 파악할 수 있게 해줍니다.

                        ### 결론:

                        셀프 어텐션은 입력 문장에서 각 단어가 다른 단어들과 얼마나 중요한 관계를 가지고 있는지 계산하여, **문맥을 더 잘 이해**하고, 
                        중요한 정보에 **더 집중**할 수 있도록 도와줍니다.
                        ''')
            p4 , p4_1 = st.columns([1,2])
            with p4:
                st.image('./img/transformers/transformer6-1.png')
            with p4_1:
                st.markdown('''
                            ### 1. Self-Attention의 연관성 매트릭스

                            이 그림은 "I am a student"라는 문장에서 **Self-Attention**이 단어들 간의 연관성을 계산하는 과정을 보여줍니다. 각 단어가 다른 단어와 얼마나 관련이 있는지 수치로 나타내고 있습니다.

                            - **Diagonal values (대각선 값들)**: 각 단어가 자기 자신과의 연관성을 나타내며, 당연히 값이 큽니다. 예를 들어, "I"와 "I"는 1.5, "student"와 "student"는 1.5로 높은 값을 갖고 있습니다.
                            - **Off-diagonal values (대각선 외 값들)**: 각 단어가 다른 단어와 얼마나 관련 있는지 나타냅니다. 
                                예를 들어, "I"와 "student"의 연관성은 1.0으로 상대적으로 높은 편입니다. 이 값을 통해 모델은 문맥적으로 관련된 단어들에 더 집중할 수 있습니다.
                            ''')
            p4_2 , p4_3 = st.columns([1,1.5])
            with p4_2:
                st.image('./img/transformers/transformer7-2.png')
            with p4_3:
                st.markdown('''
                            ### 2. 단어 간 상관성

                            이 그림은 Self-Attention이 문장에서 각 단어가 서로 어떻게 연결되는지를 보여줍니다.

                            - 예를 들어, "어제 카페에 갔어요"라는 문장에서 "카페"와 "거기"가 매우 강하게 연결된 것을 볼 수 있습니다. 
                              두 단어는 문맥상 관련이 높으므로 Self-Attention 메커니즘은 이 두 단어 사이의 연관성을 더 크게 계산합니다.
                            - 아래쪽 그림에서는 "카페"가 중요한 단어로 판단되어 다른 단어들과 연결되는 모습을 보여줍니다.

                            이 과정에서 Self-Attention은 단어 간의 상호 관련성을 파악해, 문장의 의미를 더 잘 이해할 수 있게 돕습니다.
                            ''')
            p4_4 , p4_5 = st.columns([1,3])
            with p4_4:
                st.image('./img/transformers/transformer9-1.png')
            with p4_5:
                st.markdown('''
                            ### 3. **대명사 참조 문제**

                            이 그림은 Self-Attention이 어떻게 대명사("it")가 가리키는 대상을 파악하는지를 보여줍니다.

                            - 문장 "The animal didn't cross the street because it was too tired"에서, **"it"** 이라는 대명사가 **"animal"** 을 가리키고 있음을 보여줍니다. 
                            Self-Attention 메커니즘은 "it"과 "animal" 사이의 높은 연관성을 계산하여, 모델이 문맥을 더 잘 이해할 수 있도록 도와줍니다.
                            
                            이러한 기능 덕분에 Self-Attention은 문장에서 대명사가 어떤 대상을 가리키는지 더 정확하게 파악할 수 있습니다.
                            ''')
            

